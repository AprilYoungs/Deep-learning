{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## tokenizer\n",
    "> convert the character to and from integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('history.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "check out the first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u3000卷一·五帝本纪第一\\n\\n\\u3000\\u3000黄帝者，少典之子，姓公孙，名曰轩辕。生而神灵，弱而能言，幼而徇齐，长而敦敏，成而聪明。轩辕之时，神农氏世衰。诸侯相侵伐，暴虐百姓，而神农氏弗能征。于是轩辕乃习用干戈，以征'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4774"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644266"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.95307917888562"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)/len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "encoded integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  18,   18,  506,   30,    7,   98, 1137, 1776, 3008, 2879,   30,\n",
       "          0,    0,   18,   18, 4695, 1137, 3153, 4770, 1021,  333,   70,\n",
       "        937, 4770,  874,  323,  942, 4770,  566, 1750, 3965, 3995,   20,\n",
       "       2545, 3155, 2756, 2331, 4770, 1218, 3155, 3226, 3669, 4770, 1153,\n",
       "       3155, 1249, 4721, 4770, 4280, 3155, 1634, 1623, 4770, 1437, 3155,\n",
       "       3175, 1702,   20, 3965, 3995,   70, 1692, 4770, 2756,  346, 2034,\n",
       "         45, 3590,   20, 3755,  219, 2646,  220,  156, 4770, 1745, 3491,\n",
       "       2615,  874, 4770, 3155, 2756,  346, 2034, 1206, 3226, 1245,   20,\n",
       "         94, 1714, 3965, 3995,   67,   83, 2547, 1147, 1431, 4770,  143,\n",
       "       1245], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: \"'\",\n",
       " 2: '.',\n",
       " 3: '`',\n",
       " 4: 'k',\n",
       " 5: '{',\n",
       " 6: '}',\n",
       " 7: '·',\n",
       " 8: 'Я',\n",
       " 9: '‘',\n",
       " 10: '’',\n",
       " 11: '“',\n",
       " 12: '”',\n",
       " 13: '…',\n",
       " 14: '┱',\n",
       " 15: '■',\n",
       " 16: '□',\n",
       " 17: '◎',\n",
       " 18: '\\u3000',\n",
       " 19: '、',\n",
       " 20: '。',\n",
       " 21: '々',\n",
       " 22: '《',\n",
       " 23: '》',\n",
       " 24: '䌷',\n",
       " 25: '䎬',\n",
       " 26: '䜣',\n",
       " 27: '䲡',\n",
       " 28: '䴔',\n",
       " 29: '䴖',\n",
       " 30: '一',\n",
       " 31: '丁',\n",
       " 32: '七',\n",
       " 33: '万',\n",
       " 34: '丈',\n",
       " 35: '三',\n",
       " 36: '上',\n",
       " 37: '下',\n",
       " 38: '不',\n",
       " 39: '与',\n",
       " 40: '丐',\n",
       " 41: '丑',\n",
       " 42: '专',\n",
       " 43: '且',\n",
       " 44: '丕',\n",
       " 45: '世',\n",
       " 46: '丘',\n",
       " 47: '丙',\n",
       " 48: '业',\n",
       " 49: '丛',\n",
       " 50: '东',\n",
       " 51: '丝',\n",
       " 52: '丞',\n",
       " 53: '两',\n",
       " 54: '严',\n",
       " 55: '丧',\n",
       " 56: '个',\n",
       " 57: '中',\n",
       " 58: '丰',\n",
       " 59: '临',\n",
       " 60: '丸',\n",
       " 61: '丹',\n",
       " 62: '为',\n",
       " 63: '主',\n",
       " 64: '丽',\n",
       " 65: '举',\n",
       " 66: '乂',\n",
       " 67: '乃',\n",
       " 68: '久',\n",
       " 69: '义',\n",
       " 70: '之',\n",
       " 71: '乌',\n",
       " 72: '乍',\n",
       " 73: '乎',\n",
       " 74: '乏',\n",
       " 75: '乐',\n",
       " 76: '乔',\n",
       " 77: '乖',\n",
       " 78: '乘',\n",
       " 79: '乙',\n",
       " 80: '九',\n",
       " 81: '乞',\n",
       " 82: '也',\n",
       " 83: '习',\n",
       " 84: '乡',\n",
       " 85: '书',\n",
       " 86: '买',\n",
       " 87: '乱',\n",
       " 88: '乳',\n",
       " 89: '乾',\n",
       " 90: '予',\n",
       " 91: '争',\n",
       " 92: '事',\n",
       " 93: '二',\n",
       " 94: '于',\n",
       " 95: '亏',\n",
       " 96: '云',\n",
       " 97: '互',\n",
       " 98: '五',\n",
       " 99: '井',\n",
       " 100: '亘',\n",
       " 101: '亚',\n",
       " 102: '亟',\n",
       " 103: '亡',\n",
       " 104: '亢',\n",
       " 105: '交',\n",
       " 106: '亥',\n",
       " 107: '亦',\n",
       " 108: '产',\n",
       " 109: '亨',\n",
       " 110: '亩',\n",
       " 111: '享',\n",
       " 112: '京',\n",
       " 113: '亭',\n",
       " 114: '亮',\n",
       " 115: '亲',\n",
       " 116: '亳',\n",
       " 117: '亶',\n",
       " 118: '亸',\n",
       " 119: '亹',\n",
       " 120: '人',\n",
       " 121: '亿',\n",
       " 122: '什',\n",
       " 123: '仁',\n",
       " 124: '仄',\n",
       " 125: '仅',\n",
       " 126: '仆',\n",
       " 127: '仇',\n",
       " 128: '今',\n",
       " 129: '介',\n",
       " 130: '仍',\n",
       " 131: '从',\n",
       " 132: '仑',\n",
       " 133: '仓',\n",
       " 134: '仕',\n",
       " 135: '他',\n",
       " 136: '仗',\n",
       " 137: '仙',\n",
       " 138: '仞',\n",
       " 139: '仟',\n",
       " 140: '仡',\n",
       " 141: '代',\n",
       " 142: '令',\n",
       " 143: '以',\n",
       " 144: '仪',\n",
       " 145: '仰',\n",
       " 146: '仲',\n",
       " 147: '价',\n",
       " 148: '任',\n",
       " 149: '仿',\n",
       " 150: '伉',\n",
       " 151: '伊',\n",
       " 152: '伋',\n",
       " 153: '伍',\n",
       " 154: '伎',\n",
       " 155: '伏',\n",
       " 156: '伐',\n",
       " 157: '休',\n",
       " 158: '众',\n",
       " 159: '优',\n",
       " 160: '伙',\n",
       " 161: '会',\n",
       " 162: '伛',\n",
       " 163: '伟',\n",
       " 164: '传',\n",
       " 165: '伤',\n",
       " 166: '伦',\n",
       " 167: '伪',\n",
       " 168: '伯',\n",
       " 169: '伶',\n",
       " 170: '伸',\n",
       " 171: '伺',\n",
       " 172: '似',\n",
       " 173: '佁',\n",
       " 174: '佃',\n",
       " 175: '但',\n",
       " 176: '位',\n",
       " 177: '低',\n",
       " 178: '住',\n",
       " 179: '佐',\n",
       " 180: '佑',\n",
       " 181: '体',\n",
       " 182: '佔',\n",
       " 183: '何',\n",
       " 184: '佗',\n",
       " 185: '余',\n",
       " 186: '佚',\n",
       " 187: '佛',\n",
       " 188: '作',\n",
       " 189: '佞',\n",
       " 190: '佣',\n",
       " 191: '佩',\n",
       " 192: '佯',\n",
       " 193: '佰',\n",
       " 194: '佳',\n",
       " 195: '佹',\n",
       " 196: '佺',\n",
       " 197: '佼',\n",
       " 198: '使',\n",
       " 199: '侃',\n",
       " 200: '侄',\n",
       " 201: '侈',\n",
       " 202: '例',\n",
       " 203: '侍',\n",
       " 204: '侏',\n",
       " 205: '侑',\n",
       " 206: '侔',\n",
       " 207: '侗',\n",
       " 208: '侘',\n",
       " 209: '供',\n",
       " 210: '依',\n",
       " 211: '侠',\n",
       " 212: '侣',\n",
       " 213: '侥',\n",
       " 214: '侦',\n",
       " 215: '侧',\n",
       " 216: '侨',\n",
       " 217: '侪',\n",
       " 218: '侮',\n",
       " 219: '侯',\n",
       " 220: '侵',\n",
       " 221: '便',\n",
       " 222: '俀',\n",
       " 223: '促',\n",
       " 224: '俉',\n",
       " 225: '俊',\n",
       " 226: '俎',\n",
       " 227: '俓',\n",
       " 228: '俗',\n",
       " 229: '俘',\n",
       " 230: '俛',\n",
       " 231: '保',\n",
       " 232: '俞',\n",
       " 233: '俟',\n",
       " 234: '信',\n",
       " 235: '俦',\n",
       " 236: '俨',\n",
       " 237: '俭',\n",
       " 238: '修',\n",
       " 239: '俯',\n",
       " 240: '俱',\n",
       " 241: '俳',\n",
       " 242: '俶',\n",
       " 243: '俷',\n",
       " 244: '俸',\n",
       " 245: '俾',\n",
       " 246: '倍',\n",
       " 247: '倒',\n",
       " 248: '倓',\n",
       " 249: '倔',\n",
       " 250: '倕',\n",
       " 251: '候',\n",
       " 252: '倚',\n",
       " 253: '倜',\n",
       " 254: '借',\n",
       " 255: '倡',\n",
       " 256: '倦',\n",
       " 257: '倨',\n",
       " 258: '倩',\n",
       " 259: '倪',\n",
       " 260: '倮',\n",
       " 261: '倳',\n",
       " 262: '债',\n",
       " 263: '值',\n",
       " 264: '倾',\n",
       " 265: '偁',\n",
       " 266: '偃',\n",
       " 267: '假',\n",
       " 268: '偏',\n",
       " 269: '偓',\n",
       " 270: '偕',\n",
       " 271: '健',\n",
       " 272: '偩',\n",
       " 273: '偪',\n",
       " 274: '偶',\n",
       " 275: '偷',\n",
       " 276: '偻',\n",
       " 277: '偾',\n",
       " 278: '偿',\n",
       " 279: '傅',\n",
       " 280: '傍',\n",
       " 281: '傒',\n",
       " 282: '傥',\n",
       " 283: '储',\n",
       " 284: '傲',\n",
       " 285: '僄',\n",
       " 286: '僇',\n",
       " 287: '像',\n",
       " 288: '僖',\n",
       " 289: '僚',\n",
       " 290: '僦',\n",
       " 291: '僬',\n",
       " 292: '僭',\n",
       " 293: '僮',\n",
       " 294: '僰',\n",
       " 295: '僵',\n",
       " 296: '僻',\n",
       " 297: '僿',\n",
       " 298: '儋',\n",
       " 299: '儐',\n",
       " 300: '儒',\n",
       " 301: '儗',\n",
       " 302: '儵',\n",
       " 303: '允',\n",
       " 304: '元',\n",
       " 305: '兄',\n",
       " 306: '充',\n",
       " 307: '兆',\n",
       " 308: '先',\n",
       " 309: '光',\n",
       " 310: '克',\n",
       " 311: '免',\n",
       " 312: '兑',\n",
       " 313: '兒',\n",
       " 314: '兔',\n",
       " 315: '兕',\n",
       " 316: '兗',\n",
       " 317: '党',\n",
       " 318: '兜',\n",
       " 319: '兢',\n",
       " 320: '入',\n",
       " 321: '全',\n",
       " 322: '八',\n",
       " 323: '公',\n",
       " 324: '六',\n",
       " 325: '兮',\n",
       " 326: '兰',\n",
       " 327: '共',\n",
       " 328: '关',\n",
       " 329: '兴',\n",
       " 330: '兵',\n",
       " 331: '其',\n",
       " 332: '具',\n",
       " 333: '典',\n",
       " 334: '兹',\n",
       " 335: '养',\n",
       " 336: '兼',\n",
       " 337: '兽',\n",
       " 338: '冀',\n",
       " 339: '内',\n",
       " 340: '册',\n",
       " 341: '再',\n",
       " 342: '冒',\n",
       " 343: '冕',\n",
       " 344: '写',\n",
       " 345: '军',\n",
       " 346: '农',\n",
       " 347: '冠',\n",
       " 348: '冢',\n",
       " 349: '冤',\n",
       " 350: '冥',\n",
       " 351: '冬',\n",
       " 352: '冯',\n",
       " 353: '冰',\n",
       " 354: '冲',\n",
       " 355: '决',\n",
       " 356: '况',\n",
       " 357: '冶',\n",
       " 358: '冻',\n",
       " 359: '净',\n",
       " 360: '凄',\n",
       " 361: '准',\n",
       " 362: '凉',\n",
       " 363: '凋',\n",
       " 364: '凌',\n",
       " 365: '减',\n",
       " 366: '凑',\n",
       " 367: '凛',\n",
       " 368: '凝',\n",
       " 369: '几',\n",
       " 370: '凡',\n",
       " 371: '凤',\n",
       " 372: '凫',\n",
       " 373: '凭',\n",
       " 374: '凯',\n",
       " 375: '凶',\n",
       " 376: '出',\n",
       " 377: '击',\n",
       " 378: '函',\n",
       " 379: '凿',\n",
       " 380: '刀',\n",
       " 381: '刃',\n",
       " 382: '分',\n",
       " 383: '切',\n",
       " 384: '刈',\n",
       " 385: '刍',\n",
       " 386: '刎',\n",
       " 387: '刑',\n",
       " 388: '划',\n",
       " 389: '刓',\n",
       " 390: '刖',\n",
       " 391: '列',\n",
       " 392: '刘',\n",
       " 393: '则',\n",
       " 394: '刚',\n",
       " 395: '创',\n",
       " 396: '初',\n",
       " 397: '删',\n",
       " 398: '判',\n",
       " 399: '利',\n",
       " 400: '别',\n",
       " 401: '刬',\n",
       " 402: '刭',\n",
       " 403: '刮',\n",
       " 404: '到',\n",
       " 405: '刳',\n",
       " 406: '制',\n",
       " 407: '刷',\n",
       " 408: '券',\n",
       " 409: '刺',\n",
       " 410: '刻',\n",
       " 411: '削',\n",
       " 412: '剋',\n",
       " 413: '剌',\n",
       " 414: '前',\n",
       " 415: '剑',\n",
       " 416: '剔',\n",
       " 417: '剖',\n",
       " 418: '剟',\n",
       " 419: '剡',\n",
       " 420: '剥',\n",
       " 421: '剧',\n",
       " 422: '副',\n",
       " 423: '割',\n",
       " 424: '剸',\n",
       " 425: '剽',\n",
       " 426: '剿',\n",
       " 427: '劓',\n",
       " 428: '劚',\n",
       " 429: '力',\n",
       " 430: '劝',\n",
       " 431: '办',\n",
       " 432: '功',\n",
       " 433: '加',\n",
       " 434: '务',\n",
       " 435: '动',\n",
       " 436: '助',\n",
       " 437: '劫',\n",
       " 438: '励',\n",
       " 439: '劲',\n",
       " 440: '劳',\n",
       " 441: '劾',\n",
       " 442: '势',\n",
       " 443: '勃',\n",
       " 444: '勇',\n",
       " 445: '勉',\n",
       " 446: '勋',\n",
       " 447: '勒',\n",
       " 448: '募',\n",
       " 449: '勣',\n",
       " 450: '勤',\n",
       " 451: '勺',\n",
       " 452: '勿',\n",
       " 453: '匄',\n",
       " 454: '包',\n",
       " 455: '匈',\n",
       " 456: '匍',\n",
       " 457: '匏',\n",
       " 458: '匐',\n",
       " 459: '匕',\n",
       " 460: '化',\n",
       " 461: '北',\n",
       " 462: '匝',\n",
       " 463: '匠',\n",
       " 464: '匡',\n",
       " 465: '匣',\n",
       " 466: '匪',\n",
       " 467: '匭',\n",
       " 468: '匮',\n",
       " 469: '匹',\n",
       " 470: '区',\n",
       " 471: '医',\n",
       " 472: '匽',\n",
       " 473: '匿',\n",
       " 474: '十',\n",
       " 475: '千',\n",
       " 476: '升',\n",
       " 477: '午',\n",
       " 478: '卉',\n",
       " 479: '半',\n",
       " 480: '华',\n",
       " 481: '协',\n",
       " 482: '卑',\n",
       " 483: '卒',\n",
       " 484: '卓',\n",
       " 485: '单',\n",
       " 486: '卖',\n",
       " 487: '南',\n",
       " 488: '博',\n",
       " 489: '卜',\n",
       " 490: '卞',\n",
       " 491: '占',\n",
       " 492: '卢',\n",
       " 493: '卣',\n",
       " 494: '卤',\n",
       " 495: '卦',\n",
       " 496: '卧',\n",
       " 497: '卫',\n",
       " 498: '卬',\n",
       " 499: '卮',\n",
       " 500: '卯',\n",
       " 501: '印',\n",
       " 502: '危',\n",
       " 503: '即',\n",
       " 504: '却',\n",
       " 505: '卵',\n",
       " 506: '卷',\n",
       " 507: '卻',\n",
       " 508: '卼',\n",
       " 509: '卿',\n",
       " 510: '厄',\n",
       " 511: '历',\n",
       " 512: '厉',\n",
       " 513: '压',\n",
       " 514: '厌',\n",
       " 515: '厎',\n",
       " 516: '厓',\n",
       " 517: '厕',\n",
       " 518: '厚',\n",
       " 519: '原',\n",
       " 520: '厢',\n",
       " 521: '厥',\n",
       " 522: '厦',\n",
       " 523: '厨',\n",
       " 524: '厩',\n",
       " 525: '厮',\n",
       " 526: '去',\n",
       " 527: '县',\n",
       " 528: '参',\n",
       " 529: '又',\n",
       " 530: '及',\n",
       " 531: '友',\n",
       " 532: '双',\n",
       " 533: '反',\n",
       " 534: '发',\n",
       " 535: '叔',\n",
       " 536: '取',\n",
       " 537: '受',\n",
       " 538: '变',\n",
       " 539: '叙',\n",
       " 540: '叛',\n",
       " 541: '叟',\n",
       " 542: '口',\n",
       " 543: '古',\n",
       " 544: '句',\n",
       " 545: '叩',\n",
       " 546: '只',\n",
       " 547: '叫',\n",
       " 548: '召',\n",
       " 549: '可',\n",
       " 550: '台',\n",
       " 551: '叱',\n",
       " 552: '史',\n",
       " 553: '右',\n",
       " 554: '叶',\n",
       " 555: '号',\n",
       " 556: '司',\n",
       " 557: '叹',\n",
       " 558: '叽',\n",
       " 559: '吁',\n",
       " 560: '吃',\n",
       " 561: '各',\n",
       " 562: '合',\n",
       " 563: '吉',\n",
       " 564: '吊',\n",
       " 565: '同',\n",
       " 566: '名',\n",
       " 567: '后',\n",
       " 568: '吏',\n",
       " 569: '吐',\n",
       " 570: '向',\n",
       " 571: '吒',\n",
       " 572: '吕',\n",
       " 573: '君',\n",
       " 574: '吝',\n",
       " 575: '吞',\n",
       " 576: '吟',\n",
       " 577: '吠',\n",
       " 578: '否',\n",
       " 579: '含',\n",
       " 580: '听',\n",
       " 581: '吮',\n",
       " 582: '启',\n",
       " 583: '吴',\n",
       " 584: '吸',\n",
       " 585: '吹',\n",
       " 586: '吾',\n",
       " 587: '呀',\n",
       " 588: '呈',\n",
       " 589: '告',\n",
       " 590: '呕',\n",
       " 591: '员',\n",
       " 592: '呜',\n",
       " 593: '周',\n",
       " 594: '呫',\n",
       " 595: '呰',\n",
       " 596: '味',\n",
       " 597: '呴',\n",
       " 598: '呵',\n",
       " 599: '呷',\n",
       " 600: '呼',\n",
       " 601: '命',\n",
       " 602: '咀',\n",
       " 603: '咄',\n",
       " 604: '和',\n",
       " 605: '咎',\n",
       " 606: '咏',\n",
       " 607: '咙',\n",
       " 608: '咢',\n",
       " 609: '咤',\n",
       " 610: '咨',\n",
       " 611: '咫',\n",
       " 612: '咳',\n",
       " 613: '咸',\n",
       " 614: '咺',\n",
       " 615: '咽',\n",
       " 616: '哀',\n",
       " 617: '品',\n",
       " 618: '哆',\n",
       " 619: '哉',\n",
       " 620: '响',\n",
       " 621: '哑',\n",
       " 622: '哙',\n",
       " 623: '哜',\n",
       " 624: '哥',\n",
       " 625: '哭',\n",
       " 626: '哲',\n",
       " 627: '哺',\n",
       " 628: '唆',\n",
       " 629: '唉',\n",
       " 630: '唊',\n",
       " 631: '唏',\n",
       " 632: '唐',\n",
       " 633: '唡',\n",
       " 634: '唫',\n",
       " 635: '售',\n",
       " 636: '唯',\n",
       " 637: '唱',\n",
       " 638: '唶',\n",
       " 639: '唼',\n",
       " 640: '唾',\n",
       " 641: '商',\n",
       " 642: '啐',\n",
       " 643: '啖',\n",
       " 644: '啗',\n",
       " 645: '啜',\n",
       " 646: '啬',\n",
       " 647: '啴',\n",
       " 648: '啸',\n",
       " 649: '啼',\n",
       " 650: '喁',\n",
       " 651: '善',\n",
       " 652: '喉',\n",
       " 653: '喋',\n",
       " 654: '喑',\n",
       " 655: '喘',\n",
       " 656: '喙',\n",
       " 657: '喜',\n",
       " 658: '喝',\n",
       " 659: '喟',\n",
       " 660: '喭',\n",
       " 661: '喻',\n",
       " 662: '喾',\n",
       " 663: '嗌',\n",
       " 664: '嗛',\n",
       " 665: '嗜',\n",
       " 666: '嗟',\n",
       " 667: '嗣',\n",
       " 668: '嗥',\n",
       " 669: '嗫',\n",
       " 670: '嗷',\n",
       " 671: '嗽',\n",
       " 672: '嘄',\n",
       " 673: '嘉',\n",
       " 674: '嘒',\n",
       " 675: '嘘',\n",
       " 676: '嘻',\n",
       " 677: '嘿',\n",
       " 678: '噁',\n",
       " 679: '噍',\n",
       " 680: '噏',\n",
       " 681: '噣',\n",
       " 682: '噤',\n",
       " 683: '器',\n",
       " 684: '噩',\n",
       " 685: '噪',\n",
       " 686: '噫',\n",
       " 687: '噬',\n",
       " 688: '噭',\n",
       " 689: '嚄',\n",
       " 690: '嚚',\n",
       " 691: '嚜',\n",
       " 692: '嚣',\n",
       " 693: '嚪',\n",
       " 694: '嚬',\n",
       " 695: '嚭',\n",
       " 696: '嚼',\n",
       " 697: '囊',\n",
       " 698: '囚',\n",
       " 699: '四',\n",
       " 700: '回',\n",
       " 701: '因',\n",
       " 702: '园',\n",
       " 703: '困',\n",
       " 704: '围',\n",
       " 705: '囷',\n",
       " 706: '囹',\n",
       " 707: '固',\n",
       " 708: '国',\n",
       " 709: '图',\n",
       " 710: '囿',\n",
       " 711: '圃',\n",
       " 712: '圄',\n",
       " 713: '圆',\n",
       " 714: '圈',\n",
       " 715: '圉',\n",
       " 716: '圜',\n",
       " 717: '土',\n",
       " 718: '圣',\n",
       " 719: '在',\n",
       " 720: '圩',\n",
       " 721: '圬',\n",
       " 722: '圭',\n",
       " 723: '圮',\n",
       " 724: '圯',\n",
       " 725: '地',\n",
       " 726: '场',\n",
       " 727: '坂',\n",
       " 728: '均',\n",
       " 729: '坌',\n",
       " 730: '坎',\n",
       " 731: '坏',\n",
       " 732: '坐',\n",
       " 733: '坑',\n",
       " 734: '坚',\n",
       " 735: '坛',\n",
       " 736: '坟',\n",
       " 737: '坠',\n",
       " 738: '坤',\n",
       " 739: '坫',\n",
       " 740: '坱',\n",
       " 741: '坻',\n",
       " 742: '坼',\n",
       " 743: '垂',\n",
       " 744: '垄',\n",
       " 745: '垅',\n",
       " 746: '垆',\n",
       " 747: '垒',\n",
       " 748: '垓',\n",
       " 749: '垘',\n",
       " 750: '垝',\n",
       " 751: '垠',\n",
       " 752: '垢',\n",
       " 753: '垣',\n",
       " 754: '垦',\n",
       " 755: '垩',\n",
       " 756: '埃',\n",
       " 757: '埆',\n",
       " 758: '埋',\n",
       " 759: '城',\n",
       " 760: '埏',\n",
       " 761: '埒',\n",
       " 762: '埙',\n",
       " 763: '域',\n",
       " 764: '埴',\n",
       " 765: '埶',\n",
       " 766: '培',\n",
       " 767: '基',\n",
       " 768: '埼',\n",
       " 769: '埽',\n",
       " 770: '堀',\n",
       " 771: '堂',\n",
       " 772: '堆',\n",
       " 773: '堇',\n",
       " 774: '堑',\n",
       " 775: '堕',\n",
       " 776: '堙',\n",
       " 777: '堣',\n",
       " 778: '堧',\n",
       " 779: '堪',\n",
       " 780: '堵',\n",
       " 781: '塊',\n",
       " 782: '塕',\n",
       " 783: '塞',\n",
       " 784: '填',\n",
       " 785: '塯',\n",
       " 786: '境',\n",
       " 787: '墓',\n",
       " 788: '墙',\n",
       " 789: '墝',\n",
       " 790: '增',\n",
       " 791: '墟',\n",
       " 792: '墠',\n",
       " 793: '墨',\n",
       " 794: '壁',\n",
       " 795: '壃',\n",
       " 796: '壅',\n",
       " 797: '壑',\n",
       " 798: '壖',\n",
       " 799: '壙',\n",
       " 800: '壤',\n",
       " 801: '壧',\n",
       " 802: '士',\n",
       " 803: '壬',\n",
       " 804: '壮',\n",
       " 805: '声',\n",
       " 806: '壶',\n",
       " 807: '壹',\n",
       " 808: '处',\n",
       " 809: '备',\n",
       " 810: '复',\n",
       " 811: '夏',\n",
       " 812: '夐',\n",
       " 813: '夔',\n",
       " 814: '夕',\n",
       " 815: '外',\n",
       " 816: '夙',\n",
       " 817: '多',\n",
       " 818: '夜',\n",
       " 819: '大',\n",
       " 820: '天',\n",
       " 821: '太',\n",
       " 822: '夫',\n",
       " 823: '夬',\n",
       " 824: '夭',\n",
       " 825: '央',\n",
       " 826: '失',\n",
       " 827: '头',\n",
       " 828: '夷',\n",
       " 829: '夸',\n",
       " 830: '夹',\n",
       " 831: '夺',\n",
       " 832: '奂',\n",
       " 833: '奄',\n",
       " 834: '奇',\n",
       " 835: '奈',\n",
       " 836: '奉',\n",
       " 837: '奋',\n",
       " 838: '奎',\n",
       " 839: '奏',\n",
       " 840: '契',\n",
       " 841: '奔',\n",
       " 842: '奕',\n",
       " 843: '奚',\n",
       " 844: '奠',\n",
       " 845: '奡',\n",
       " 846: '奢',\n",
       " 847: '奥',\n",
       " 848: '奭',\n",
       " 849: '女',\n",
       " 850: '奴',\n",
       " 851: '奸',\n",
       " 852: '好',\n",
       " 853: '如',\n",
       " 854: '妃',\n",
       " 855: '妄',\n",
       " 856: '妇',\n",
       " 857: '妒',\n",
       " 858: '妖',\n",
       " 859: '妙',\n",
       " 860: '妤',\n",
       " 861: '妨',\n",
       " 862: '妩',\n",
       " 863: '妪',\n",
       " 864: '妫',\n",
       " 865: '妲',\n",
       " 866: '妹',\n",
       " 867: '妻',\n",
       " 868: '妾',\n",
       " 869: '姁',\n",
       " 870: '始',\n",
       " 871: '姐',\n",
       " 872: '姑',\n",
       " 873: '姒',\n",
       " 874: '姓',\n",
       " 875: '委',\n",
       " 876: '姚',\n",
       " 877: '姜',\n",
       " 878: '姞',\n",
       " 879: '姣',\n",
       " 880: '姬',\n",
       " 881: '姺',\n",
       " 882: '姻',\n",
       " 883: '姿',\n",
       " 884: '娀',\n",
       " 885: '威',\n",
       " 886: '娃',\n",
       " 887: '娄',\n",
       " 888: '娉',\n",
       " 889: '娖',\n",
       " 890: '娙',\n",
       " 891: '娟',\n",
       " 892: '娠',\n",
       " 893: '娣',\n",
       " 894: '娥',\n",
       " 895: '娱',\n",
       " 896: '娲',\n",
       " 897: '娵',\n",
       " 898: '娶',\n",
       " 899: '婉',\n",
       " 900: '婕',\n",
       " 901: '婘',\n",
       " 902: '婚',\n",
       " 903: '婢',\n",
       " 904: '婴',\n",
       " 905: '婺',\n",
       " 906: '婿',\n",
       " 907: '媒',\n",
       " 908: '媚',\n",
       " 909: '媢',\n",
       " 910: '媪',\n",
       " 911: '媵',\n",
       " 912: '媻',\n",
       " 913: '媾',\n",
       " 914: '媿',\n",
       " 915: '嫁',\n",
       " 916: '嫂',\n",
       " 917: '嫄',\n",
       " 918: '嫈',\n",
       " 919: '嫉',\n",
       " 920: '嫌',\n",
       " 921: '嫖',\n",
       " 922: '嫘',\n",
       " 923: '嫚',\n",
       " 924: '嫡',\n",
       " 925: '嫣',\n",
       " 926: '嫪',\n",
       " 927: '嫳',\n",
       " 928: '嫺',\n",
       " 929: '嬃',\n",
       " 930: '嬉',\n",
       " 931: '嬐',\n",
       " 932: '嬓',\n",
       " 933: '嬖',\n",
       " 934: '嬗',\n",
       " 935: '嬛',\n",
       " 936: '嬴',\n",
       " 937: '子',\n",
       " 938: '孔',\n",
       " 939: '孕',\n",
       " 940: '字',\n",
       " 941: '存',\n",
       " 942: '孙',\n",
       " 943: '孛',\n",
       " 944: '孜',\n",
       " 945: '孝',\n",
       " 946: '孟',\n",
       " 947: '季',\n",
       " 948: '孤',\n",
       " 949: '孥',\n",
       " 950: '学',\n",
       " 951: '孰',\n",
       " 952: '孱',\n",
       " 953: '孳',\n",
       " 954: '孺',\n",
       " 955: '孽',\n",
       " 956: '宁',\n",
       " 957: '它',\n",
       " 958: '宄',\n",
       " 959: '宅',\n",
       " 960: '宇',\n",
       " 961: '守',\n",
       " 962: '安',\n",
       " 963: '宋',\n",
       " 964: '完',\n",
       " 965: '宏',\n",
       " 966: '宓',\n",
       " 967: '宗',\n",
       " 968: '官',\n",
       " 969: '宙',\n",
       " 970: '定',\n",
       " 971: '宛',\n",
       " 972: '宜',\n",
       " 973: '宝',\n",
       " 974: '实',\n",
       " 975: '宠',\n",
       " 976: '审',\n",
       " 977: '客',\n",
       " 978: '宣',\n",
       " 979: '室',\n",
       " 980: '宥',\n",
       " 981: '宦',\n",
       " 982: '宧',\n",
       " 983: '宪',\n",
       " 984: '宫',\n",
       " 985: '宰',\n",
       " 986: '害',\n",
       " 987: '宵',\n",
       " 988: '家',\n",
       " 989: '容',\n",
       " 990: '宽',\n",
       " 991: '宾',\n",
       " 992: '宿',\n",
       " 993: '寂',\n",
       " 994: '寄',\n",
       " 995: '寅',\n",
       " 996: '密',\n",
       " 997: '寇',\n",
       " 998: '富',\n",
       " 999: '寐',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training mini-batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''\n",
    "    Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    characters_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    arr = arr[:characters_per_batch*n_batches]\n",
    "    \n",
    "    arr = np.reshape(arr,[batch_size,-1])\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y_temp = arr[:,n+1:n+n_steps+1]\n",
    "        \n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batchs = get_batches(encoded, 5, 10)\n",
    "x, y = next(batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  18   18  506   30    7   98 1137 1776 3008 2879]\n",
      " [   0   18   18   98 2002 1707 4770  376 1988   50]\n",
      " [4326   70  461 4770  143   50   59 3707 4770  487]\n",
      " [3155 3042 1681 1119   70 2043 4770  331  442 1269]\n",
      " [1626  133 2945 4770 4331 1050 2089   70 4350  143]]\n",
      "[[  18  506   30    7   98 1137 1776 3008 2879   30]\n",
      " [  18   18   98 2002 1707 4770  376 1988   50   50]\n",
      " [  70  461 4770  143   50   59 3707 4770  487  708]\n",
      " [3042 1681 1119   70 2043 4770  331  442 1269 1681]\n",
      " [ 133 2945 4770 4331 1050 2089   70 4350  143  142]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name= 'targets')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lstm cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RNN Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    '''\n",
    "        Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    '''\n",
    "    \n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size,out_size],stddev=1.0))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_shaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_shaped, logits=logits)\n",
    "    loss = tf.reduce_mean(loss, name='loss')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                lstm_size=128, num_layers=2, learning_rate=0.001,\n",
    "                grad_clip=5, sampling=False):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstm_size = 256         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/17...  Training Step: 50...  Training loss: 4.6356...  0.1743 sec/batch\n",
      "Epoch: 1/17...  Training Step: 100...  Training loss: 4.5091...  0.1754 sec/batch\n",
      "Epoch: 1/17...  Training Step: 150...  Training loss: 4.5569...  0.1753 sec/batch\n",
      "Epoch: 1/17...  Training Step: 200...  Training loss: 4.5951...  0.1754 sec/batch\n",
      "Epoch: 1/17...  Training Step: 250...  Training loss: 4.5013...  0.1756 sec/batch\n",
      "Epoch: 1/17...  Training Step: 300...  Training loss: 4.4829...  0.1759 sec/batch\n",
      "Epoch: 1/17...  Training Step: 350...  Training loss: 4.4728...  0.1760 sec/batch\n",
      "Epoch: 1/17...  Training Step: 400...  Training loss: 4.6090...  0.1737 sec/batch\n",
      "Epoch: 1/17...  Training Step: 450...  Training loss: 4.4806...  0.1755 sec/batch\n",
      "Epoch: 1/17...  Training Step: 500...  Training loss: 4.4709...  0.1762 sec/batch\n",
      "Epoch: 1/17...  Training Step: 550...  Training loss: 4.4115...  0.1763 sec/batch\n",
      "Epoch: 1/17...  Training Step: 600...  Training loss: 4.3232...  0.1738 sec/batch\n",
      "Epoch: 2/17...  Training Step: 650...  Training loss: 4.4822...  0.1749 sec/batch\n",
      "Epoch: 2/17...  Training Step: 700...  Training loss: 4.7313...  0.1737 sec/batch\n",
      "Epoch: 2/17...  Training Step: 750...  Training loss: 4.5081...  0.1733 sec/batch\n",
      "Epoch: 2/17...  Training Step: 800...  Training loss: 4.3969...  0.1725 sec/batch\n",
      "Epoch: 2/17...  Training Step: 850...  Training loss: 4.5152...  0.1745 sec/batch\n",
      "Epoch: 2/17...  Training Step: 900...  Training loss: 4.2944...  0.1763 sec/batch\n",
      "Epoch: 2/17...  Training Step: 950...  Training loss: 4.4246...  0.1769 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1000...  Training loss: 4.4933...  0.1747 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1050...  Training loss: 4.5863...  0.1757 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1100...  Training loss: 4.1123...  0.1740 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1150...  Training loss: 4.4498...  0.1762 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1200...  Training loss: 4.2744...  0.1759 sec/batch\n",
      "Epoch: 2/17...  Training Step: 1250...  Training loss: 4.2784...  0.1746 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1300...  Training loss: 4.2327...  0.1762 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1350...  Training loss: 4.2965...  0.1742 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1400...  Training loss: 4.5215...  0.1766 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1450...  Training loss: 4.3758...  0.1772 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1500...  Training loss: 4.3509...  0.1745 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1550...  Training loss: 4.4492...  0.1761 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1600...  Training loss: 4.4497...  0.1755 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1650...  Training loss: 4.5705...  0.1764 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1700...  Training loss: 4.2705...  0.1753 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1750...  Training loss: 4.1130...  0.1772 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1800...  Training loss: 4.4687...  0.1762 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1850...  Training loss: 4.2884...  0.1770 sec/batch\n",
      "Epoch: 3/17...  Training Step: 1900...  Training loss: 4.1238...  0.1767 sec/batch\n",
      "Epoch: 4/17...  Training Step: 1950...  Training loss: 4.2112...  0.1741 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2000...  Training loss: 4.3666...  0.1755 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2050...  Training loss: 4.5434...  0.1751 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2100...  Training loss: 4.4688...  0.1759 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2150...  Training loss: 4.0881...  0.1761 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2200...  Training loss: 4.1837...  0.1727 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2250...  Training loss: 4.4359...  0.1766 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2300...  Training loss: 4.2239...  0.1752 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2350...  Training loss: 4.1153...  0.1750 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2400...  Training loss: 3.9968...  0.1746 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2450...  Training loss: 4.1306...  0.1764 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2500...  Training loss: 4.2248...  0.1771 sec/batch\n",
      "Epoch: 4/17...  Training Step: 2550...  Training loss: 4.1806...  0.1758 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2600...  Training loss: 4.5566...  0.1755 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2650...  Training loss: 4.3493...  0.1747 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2700...  Training loss: 4.2780...  0.1765 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2750...  Training loss: 4.2658...  0.1760 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2800...  Training loss: 3.9516...  0.1743 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2850...  Training loss: 4.1702...  0.1746 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2900...  Training loss: 4.1185...  0.1759 sec/batch\n",
      "Epoch: 5/17...  Training Step: 2950...  Training loss: 4.1032...  0.1756 sec/batch\n",
      "Epoch: 5/17...  Training Step: 3000...  Training loss: 3.8663...  0.1766 sec/batch\n",
      "Epoch: 5/17...  Training Step: 3050...  Training loss: 4.0127...  0.1760 sec/batch\n",
      "Epoch: 5/17...  Training Step: 3100...  Training loss: 4.1449...  0.1756 sec/batch\n",
      "Epoch: 5/17...  Training Step: 3150...  Training loss: 3.9985...  0.1768 sec/batch\n",
      "Epoch: 5/17...  Training Step: 3200...  Training loss: 4.1650...  0.1743 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3250...  Training loss: 3.8723...  0.1734 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3300...  Training loss: 4.0930...  0.1768 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3350...  Training loss: 4.1287...  0.1763 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3400...  Training loss: 4.2452...  0.1759 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3450...  Training loss: 4.0653...  0.1754 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3500...  Training loss: 3.9379...  0.1767 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3550...  Training loss: 4.0122...  0.1764 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3600...  Training loss: 3.9712...  0.1755 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3650...  Training loss: 4.0553...  0.1755 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3700...  Training loss: 4.0728...  0.1742 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3750...  Training loss: 4.0866...  0.1771 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3800...  Training loss: 3.9523...  0.1755 sec/batch\n",
      "Epoch: 6/17...  Training Step: 3850...  Training loss: 4.1498...  0.1746 sec/batch\n",
      "Epoch: 7/17...  Training Step: 3900...  Training loss: 4.1303...  0.1766 sec/batch\n",
      "Epoch: 7/17...  Training Step: 3950...  Training loss: 4.2829...  0.1738 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4000...  Training loss: 3.9389...  0.1752 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4050...  Training loss: 4.0473...  0.1746 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4100...  Training loss: 4.1483...  0.1759 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4150...  Training loss: 3.9631...  0.1763 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4200...  Training loss: 4.1901...  0.1737 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4250...  Training loss: 4.4727...  0.1728 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4300...  Training loss: 3.9893...  0.1766 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4350...  Training loss: 4.0947...  0.1766 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4400...  Training loss: 3.9553...  0.1745 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4450...  Training loss: 3.8417...  0.1728 sec/batch\n",
      "Epoch: 7/17...  Training Step: 4500...  Training loss: 4.0155...  0.1757 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4550...  Training loss: 4.0571...  0.1745 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4600...  Training loss: 4.0782...  0.1743 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4650...  Training loss: 4.2094...  0.1762 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4700...  Training loss: 4.2290...  0.1759 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4750...  Training loss: 4.0803...  0.1756 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4800...  Training loss: 3.7084...  0.1743 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4850...  Training loss: 3.8912...  0.1750 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4900...  Training loss: 4.1108...  0.1762 sec/batch\n",
      "Epoch: 8/17...  Training Step: 4950...  Training loss: 3.8964...  0.1742 sec/batch\n",
      "Epoch: 8/17...  Training Step: 5000...  Training loss: 4.0467...  0.1762 sec/batch\n",
      "Epoch: 8/17...  Training Step: 5050...  Training loss: 3.9739...  0.1753 sec/batch\n",
      "Epoch: 8/17...  Training Step: 5100...  Training loss: 3.9081...  0.1758 sec/batch\n",
      "Epoch: 8/17...  Training Step: 5150...  Training loss: 4.1818...  0.1725 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5200...  Training loss: 3.9381...  0.1738 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5250...  Training loss: 3.9568...  0.1766 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5300...  Training loss: 4.0450...  0.1742 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5350...  Training loss: 4.1933...  0.1763 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5400...  Training loss: 4.0663...  0.1761 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5450...  Training loss: 3.9751...  0.1752 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5500...  Training loss: 3.9229...  0.1747 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5550...  Training loss: 3.9283...  0.1755 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5600...  Training loss: 3.8421...  0.1754 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5650...  Training loss: 4.0146...  0.1736 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5700...  Training loss: 3.7828...  0.1768 sec/batch\n",
      "Epoch: 9/17...  Training Step: 5750...  Training loss: 3.6963...  0.1755 sec/batch\n",
      "Epoch: 10/17...  Training Step: 5800...  Training loss: 3.9417...  0.1754 sec/batch\n",
      "Epoch: 10/17...  Training Step: 5850...  Training loss: 4.1106...  0.1766 sec/batch\n",
      "Epoch: 10/17...  Training Step: 5900...  Training loss: 4.1080...  0.1755 sec/batch\n",
      "Epoch: 10/17...  Training Step: 5950...  Training loss: 4.0608...  0.1752 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6000...  Training loss: 4.0058...  0.1748 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6050...  Training loss: 3.8424...  0.1747 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6100...  Training loss: 3.6707...  0.1761 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6150...  Training loss: 4.0192...  0.1744 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6200...  Training loss: 3.8309...  0.1745 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6250...  Training loss: 3.8699...  0.1740 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6300...  Training loss: 3.6572...  0.1733 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6350...  Training loss: 3.8085...  0.1746 sec/batch\n",
      "Epoch: 10/17...  Training Step: 6400...  Training loss: 3.8690...  0.1763 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6450...  Training loss: 3.9046...  0.1754 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6500...  Training loss: 3.8790...  0.1739 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6550...  Training loss: 4.1433...  0.1751 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6600...  Training loss: 3.9975...  0.1766 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6650...  Training loss: 4.1062...  0.1764 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6700...  Training loss: 4.1768...  0.1761 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6750...  Training loss: 4.0814...  0.1762 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6800...  Training loss: 3.7537...  0.1746 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6850...  Training loss: 3.9795...  0.1760 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6900...  Training loss: 3.7040...  0.1759 sec/batch\n",
      "Epoch: 11/17...  Training Step: 6950...  Training loss: 3.9528...  0.1761 sec/batch\n",
      "Epoch: 11/17...  Training Step: 7000...  Training loss: 3.7580...  0.1757 sec/batch\n",
      "Epoch: 11/17...  Training Step: 7050...  Training loss: 3.9476...  0.1761 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7100...  Training loss: 3.7331...  0.1740 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7150...  Training loss: 4.0063...  0.1751 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7200...  Training loss: 3.9959...  0.1762 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7250...  Training loss: 3.9209...  0.1748 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7300...  Training loss: 3.9106...  0.1765 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7350...  Training loss: 3.9264...  0.1734 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7400...  Training loss: 3.8735...  0.1742 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7450...  Training loss: 3.9192...  0.1725 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7500...  Training loss: 3.7251...  0.1761 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7550...  Training loss: 3.6004...  0.1746 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7600...  Training loss: 3.9453...  0.1765 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7650...  Training loss: 3.7275...  0.1741 sec/batch\n",
      "Epoch: 12/17...  Training Step: 7700...  Training loss: 3.5808...  0.1760 sec/batch\n",
      "Epoch: 13/17...  Training Step: 7750...  Training loss: 4.1011...  0.1729 sec/batch\n",
      "Epoch: 13/17...  Training Step: 7800...  Training loss: 3.8630...  0.1753 sec/batch\n",
      "Epoch: 13/17...  Training Step: 7850...  Training loss: 4.0340...  0.1758 sec/batch\n",
      "Epoch: 13/17...  Training Step: 7900...  Training loss: 4.0619...  0.1763 sec/batch\n",
      "Epoch: 13/17...  Training Step: 7950...  Training loss: 3.8942...  0.1729 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8000...  Training loss: 3.7296...  0.1753 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8050...  Training loss: 3.7138...  0.1758 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8100...  Training loss: 3.9721...  0.1741 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8150...  Training loss: 3.8238...  0.1755 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8200...  Training loss: 3.5704...  0.1754 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8250...  Training loss: 3.9065...  0.1752 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8300...  Training loss: 3.7495...  0.1764 sec/batch\n",
      "Epoch: 13/17...  Training Step: 8350...  Training loss: 3.8220...  0.1746 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8400...  Training loss: 4.0253...  0.1754 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8450...  Training loss: 3.9641...  0.1764 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8500...  Training loss: 3.8505...  0.1761 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8550...  Training loss: 3.8695...  0.1762 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8600...  Training loss: 3.8642...  0.1762 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8650...  Training loss: 3.5508...  0.1756 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8700...  Training loss: 3.7502...  0.1762 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8750...  Training loss: 3.7033...  0.1765 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8800...  Training loss: 3.5988...  0.1766 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8850...  Training loss: 3.7099...  0.1754 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8900...  Training loss: 3.8794...  0.1761 sec/batch\n",
      "Epoch: 14/17...  Training Step: 8950...  Training loss: 3.9207...  0.1765 sec/batch\n",
      "Epoch: 14/17...  Training Step: 9000...  Training loss: 3.6907...  0.1735 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9050...  Training loss: 3.8420...  0.1756 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9100...  Training loss: 3.8293...  0.1744 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9150...  Training loss: 3.7331...  0.1766 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9200...  Training loss: 3.7700...  0.1757 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9250...  Training loss: 3.8714...  0.1764 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9300...  Training loss: 3.4798...  0.1761 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9350...  Training loss: 3.8431...  0.1770 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9400...  Training loss: 3.8172...  0.1742 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9450...  Training loss: 3.4501...  0.1770 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9500...  Training loss: 3.7319...  0.1766 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9550...  Training loss: 3.7447...  0.1764 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9600...  Training loss: 3.7344...  0.1727 sec/batch\n",
      "Epoch: 15/17...  Training Step: 9650...  Training loss: 3.8197...  0.1742 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9700...  Training loss: 3.7512...  0.1761 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9750...  Training loss: 3.7847...  0.1748 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9800...  Training loss: 3.8408...  0.1756 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9850...  Training loss: 4.0001...  0.1747 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9900...  Training loss: 3.7861...  0.1744 sec/batch\n",
      "Epoch: 16/17...  Training Step: 9950...  Training loss: 3.7983...  0.1771 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10000...  Training loss: 3.7976...  0.1755 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10050...  Training loss: 3.9266...  0.1758 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10100...  Training loss: 3.5384...  0.1748 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10150...  Training loss: 3.7974...  0.1768 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10200...  Training loss: 3.8113...  0.1756 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10250...  Training loss: 3.7639...  0.1758 sec/batch\n",
      "Epoch: 16/17...  Training Step: 10300...  Training loss: 3.7806...  0.1736 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10350...  Training loss: 3.8122...  0.1723 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10400...  Training loss: 3.6556...  0.1755 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10450...  Training loss: 3.8392...  0.1740 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10500...  Training loss: 3.8913...  0.1764 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10550...  Training loss: 3.7190...  0.1740 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10600...  Training loss: 3.7046...  0.1754 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10650...  Training loss: 3.7150...  0.1763 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10700...  Training loss: 3.8931...  0.1754 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10750...  Training loss: 3.7458...  0.1753 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10800...  Training loss: 3.5067...  0.1767 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10850...  Training loss: 3.5817...  0.1760 sec/batch\n",
      "Epoch: 17/17...  Training Step: 10900...  Training loss: 3.6321...  0.1730 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 17\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "    saver.restore(sess, checkpoint)\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"west_checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"west_checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"west_checkpoints/i10948_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i1000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i1200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i1400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i1600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i1800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i2200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i2400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i2600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i2800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i3000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i3200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i3400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i3600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i3800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i4000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i4200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i4400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i4600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i4800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i5000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i5200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i5400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i5600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i5800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i6200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i6400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i6600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i6800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i7000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i7200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i7400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i7600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i7800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i8000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i8200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i8400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i8600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i8800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i9000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i9200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i9400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i9600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i9800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"west_checkpoints/i10948_l256.ckpt\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('west_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'west_checkpoints/i10948_l256.ckpt'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('west_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孙、魏之属。\n",
      "\n",
      "　　齐王之时，秦人以为郎，以故上书曰：“臣闻大臣不忠，不能以其功，故不敢以故不能得。今君王之为天子之所以为人臣，然不能自为以故为臣，故以为不可。且王之为不能不得于天下之士，以天下为大将，臣不敢以为臣也。且臣之事之所以不敢不得以王之，而以为王也。臣闻之，以故为臣之计，以为臣之事也。臣闻之，以天下之国也。\n",
      "\n",
      "　　“秦王之所与者，以韩、魏、魏、魏、魏之所以不与魏也，秦以兵攻秦，则韩、魏、魏、赵、魏、韩、魏、赵之地也，不如为秦也。且秦之彊，秦必有兵于秦，韩、魏之兵而彊于秦，则秦攻之而不可。秦以攻魏，魏不能出秦而西地，秦之彊国则秦、赵之地，以秦之兵以西割地以攻韩，韩之兵必可与秦也。秦、魏之攻齐、魏、魏而取秦也，则楚之地不敢与楚也。秦必不听秦也。秦以兵攻楚，则魏以其为大王也。今王不能与秦攻楚，而韩、魏以兵以为楚也。”秦使王翦、韩、魏、赵、魏、韩、魏、韩、魏、魏、魏、魏、魏共伐秦。三十四年，韩、魏、赵、魏、赵、魏、韩、魏、秦、魏、韩、魏、魏、秦、魏、魏、魏共伐魏，取魏阳。十二年，秦伐魏，虏楚将，拔之。\n",
      "\n",
      "　　十二年，秦拔我三城。二年，秦拔我二年。三十四年，秦拔我阳城。二十一年，与秦攻秦，秦取我中城。十四年，秦取我河东，与魏、魏、赵、韩、魏、魏、赵、魏、魏、赵、韩、魏、魏、魏共共攻魏，拔魏。\n",
      "\n",
      "　　十一年，与韩、魏会河南，以韩、魏、魏、魏、魏、魏、魏、魏、秦、魏、赵、魏、赵、魏、韩、魏、赵、魏、魏共攻韩、魏，取河、河、魏、魏、魏、韩、魏，取魏之城。四年，秦拔我城阳，虏赵将将。\n",
      "\n",
      "　　十二年，秦攻魏，取三河。三年，与魏、魏会魏。魏献十二年，与魏攻赵，拔赵。二十四年，齐与赵、赵共击秦，取其城，取赵。十二年，与魏伐秦。十四年，赵复攻魏。秦拔我十二城，取三县，秦兵复出。\n",
      "\n",
      "　　二十六年，楚败魏于澮。与韩、赵、赵、韩、魏、赵、赵、赵、韩、魏、赵、魏、魏、赵、魏、韩、魏、齐、魏、魏、魏、韩、魏、齐共伐魏，拔之。十五年，与秦会临晋。十六年，秦攻魏，拔之。二十四年，秦伐魏，取我阳城。四年，魏与魏、魏共攻赵，取魏、蒲。十五年，秦伐我。十一年，齐与韩、魏伐魏。十二年，与魏攻魏，取河上，取魏阳。十一年，秦拔我城。十四年，秦攻韩、魏，取之。十二年，秦拔我城。十五年，秦败我军于城。二年，秦围我阳城，取魏地。三十二年，秦拔我城阳。二十六年，秦拔我城，取魏城。二十五年，魏、韩、魏、魏、魏、赵、赵、魏为秦、秦、魏、魏。魏献其十二年，秦灭韩、魏。\n",
      "\n",
      "　　六年，秦败我于曲沃。十一年，与魏战于澮。十一年，秦攻赵、魏，破之，取三城。十五年，秦拔我赵。十二年，秦攻魏，取赵地。十六年，秦败我于澮。三十二年卒，弟怀王立。\n",
      "\n",
      "　　惠王十一年，与赵会临晋。三年，韩、魏、楚共伐韩，取其河中。四年，与秦会魏。十二年，秦拔我二年。十二年，齐拔我城。二十六年，王拔我三年。十四年，秦拔我阳城。十四年，秦伐我。十一年，秦拔我城。二十一年，秦拔我城，与楚王会。十一年，秦拔我城，取三阳。十四年，赵复伐韩，取三城。十一年，秦攻我阳城，虏魏将将，取赵城。三十一年，魏拔我蔺、四年。十四年，秦拔我城。十一年，秦攻韩，秦败我于焦、栎阳。二十一年，赵与魏、赵。十二年，与秦会临晋。十二年，秦败我蔺。十五年，秦伐我，虏魏王。二十一年，秦拔我二城。十四年，秦拔我三年。二十五年，秦拔我城。十二年，与赵、魏，取河、河、河、东郡、二十六、九县，秦攻韩之中城，杀其军魏。十五年，魏、魏与魏会韩、魏、赵，取河、河、济、河间、河、河、南、河、河、河之地。以魏、魏、魏、魏、魏之中、韩、魏、魏、齐、赵、魏、魏、韩、魏、魏、韩、魏、魏共伐魏，败我赵贲，虏魏公。二十四年，与韩、魏、秦相攻。十年，攻秦，取魏城。二十二年，攻齐，取魏城。十四年，秦拔我蔺、五十一。二十一年，秦拔我城。五年，魏、韩、魏、赵共攻赵，取三城。\n",
      "\n",
      "　　二十一年，秦败我三十城。十五年，秦伐我，拔我城阳。二十四年，楚复攻赵，秦拔我邯郸。\n",
      "\n",
      "　　二十五年，秦拔我城。十二年，魏拔我城。二十二年，秦拔我三城。十二年，与秦攻我城阳，取其城。二十五年，攻魏，拔之。三十四年，秦攻魏。秦拔我城阳。十四年，秦拔我城。五十四年，秦取我阳城，斩首二十五万，秦将二十二年，与秦会秦。\n",
      "\n",
      "　　十四年，秦攻魏，取三城。二十一年，秦攻魏，取之。二十二年，与魏战，取魏。二年，秦拔我三年。十四年，秦拔我阳城。三十一年，秦攻魏，取三城。二十一年，秦攻秦，拔魏城城。二十四年，攻赵，拔魏。三年，秦攻魏，取我阳城。四月，拔魏。二十四年，秦拔我城阳。二年，秦拔我三城。二十二年，秦拔我蔺，虏二十六人。十一年，秦拔我阳城。六年，攻魏，拔魏城阳。二十五年，魏与魏、魏攻魏，虏其将屈匄，斩首二十五城。五十四年，与魏攻魏，拔我城阳。十四年，秦攻魏，拔之。二十二年，秦拔我城城。二十五年，秦伐我，取我城。攻魏，取魏城。十六年，与\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"孙\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轩辕，故王夫人女为子弟。\n",
      "\n",
      "　　太史公曰：孔氏之言“有道之方，今其有道者，有所谓之所以以为人也。”于是曰：“古者之先人之有德，其人有不有，不能用其德，则有所用之。”\n",
      "\n",
      "　　其明年，秦伐齐，至于晋陵。十五年，楚败楚，遂取楚，取三河。二十六年，秦伐楚。三年，秦围我邯郸。二十一年，秦伐我，虏楚王。十四年，与魏攻韩而取魏。四年，秦拔我阳城，取三晋。二十一年，与楚攻赵，拔之。二十四年，秦拔我魏城。二十一年，秦攻我阳城。十六年，秦拔我城阳。二十二年，秦攻魏，拔我城城。二年，秦拔我三城，斩首三十三县。三十一年，秦拔我城阳二十一。三十四年，秦拔我城阳。十五年，秦拔我魏，拔我三城。二十六年，秦拔我三年。\n",
      "\n",
      "　　四年，与秦战。三十六年，与魏王会临晋。十四年，伐魏，虏楚将于芒卯，十五年，取我中城。三十一年，秦拔我三城，斩首三千，秦攻我城。二十一年，与秦战，虏魏王。赵、赵、魏、魏、魏、魏、魏、赵、魏、魏、韩、赵、魏、韩、魏、韩、魏、赵、魏、魏、魏为赵、韩为韩；韩、魏、魏、魏以兵为西南，攻魏，取赵、赵，取赵，取魏，取魏。魏取赵，赵、赵。十四年，秦伐我。十一年，攻魏，取河东，虏其将赵氏。十年，与魏、赵相距。十六年，秦拔我三年，取魏城。十二年，秦拔我二年。秦拔我二年。十五年，秦拔我城阳，取魏地。\n",
      "\n",
      "　　三十四年，秦拔我三年。\n",
      "\n",
      "　　二十六年，秦攻我邯郸，破赵。魏败我焦、曲沃。二十三年，与魏、魏、赵、魏、魏、魏、魏、魏、魏、魏、魏、魏、魏共共攻魏，拔之。四年，秦拔我城中。二十五年，秦拔我城城。十五年，魏拔我蔺。十四年，与魏、魏、韩、魏、赵、魏、魏、魏、秦共攻赵，取我城阳。四年，魏、魏、魏以韩、魏为魏。赵与魏之战，虏魏王，虏其将魏将。二十四年，与魏战，拔魏。十一年，秦拔我二城，取魏阳城。三十一年，与秦战，败我军于城下。二十六年，秦拔魏，取三城。\n",
      "\n",
      "　　二年，攻魏，取赵。二十一年，魏攻韩、魏，取我城阳。三十四年，秦拔我城阳。二十二年，魏拔我我阳阳。二十一年，秦拔我三年。\n",
      "\n",
      "　　十四年，秦攻我。魏王初立，秦使赵围邯郸。秦拔我二城。三年，秦伐赵，取我城。十二年，魏、魏、魏攻魏，取河上。十二年，秦取我河外。四月，秦拔我三城。二十五年，秦败我军，斩首五十五。十四年，秦攻赵，虏韩王信，攻秦，取三河。\n",
      "\n",
      "　　二十三年，秦拔我蔺，取魏。二十一年，秦拔我二十四城。十六年，秦拔我蔺。四年，秦拔我城城。十五年，攻秦、魏，取之。\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"轩辕\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我为人，以为人，无所以得。其所为者，皆以为大人也。其后有二世，而王、魏、魏之所以为秦也。\n",
      "\n",
      "　　秦昭王以其为大夫，秦人皆与魏王共攻秦，秦兵败，遂亡走赵。\n",
      "\n",
      "　　赵昭襄王卒，子平陵君立。\n",
      "\n",
      "　　惠王十年，齐田单弑其君悼襄王，十二年卒也，子立四年卒。二年，秦拔我城城。二年，秦攻我，拔我三城。十四年，秦拔魏二十城。\n",
      "\n",
      "　　二十一年，秦拔我三城。十二年，秦拔我二城。十四年，秦拔我城阳。十六年，秦败我三十六郡。四年，秦拔我阳城。二十五年，秦拔我魏、赵。十四年，秦攻魏。十二年，秦拔我三城。四年，与魏攻魏，取魏城。二年，与韩、魏、赵、魏、赵、韩、魏共攻秦，败秦师。十四年，攻魏、魏，取之。十五年，攻秦，取三河；东至河、魏，虏赵王卬。二十四年，与秦战，虏韩相与战。十一年，秦拔我阳城，取二城，取赵地。十二年，秦败我蔺。十六年，攻魏，虏魏王。赵与秦战，取之。十二年，攻赵，取魏。二十四年，攻齐、魏，取河、河。\n",
      "\n",
      "　　十五年，与魏、韩、韩、魏共攻魏，拔其城阳。二十六年，齐败我我城阳。十九年，齐拔我三城。十二年，秦拔我三城，取我阳阳。二年，秦伐我，拔我榆次。三十六年，攻秦，破之，拔我城。二十六年，齐攻赵。十二年，秦拔我魏。\n",
      "\n",
      "　　十一年，赵拔我魏，取三城。十一年，赵攻韩，拔之，斩首四十四万。十二年，秦拔我二十城。十二年，秦拔我城。五十六年，秦拔我二城。三十一年，秦伐我，拔之。二十一年，与秦攻韩而取魏。三十一年，秦拔魏城。三年，攻魏。秦败我于城城。十二年，秦拔我三年。十六年，秦攻我，魏拔我城。二年，秦攻我，拔之城阳。十二年，与魏伐韩，拔我陉山，取五城，斩首四万，取魏城邑。十四年，韩、魏、赵、魏、魏、魏、齐、韩、魏、魏共攻赵，魏败赵于焦、焦，取赵、济、南地、河内、东郡。二十四年，秦拔我阳城、赵。十四年，秦拔我二城。十二年，秦拔我三城。\n",
      "\n",
      "　　十四年，魏、韩、魏、魏、齐、魏共伐魏，拔之。十二年，秦拔我二城。十四年，秦伐我，败魏城。十四年，秦拔我蔺。二十六年，秦拔我三城，虏楚将军，与秦兵，攻魏。魏将十四年，秦攻我邯郸。十五年，秦败我于城阳。十二年，秦拔我阳城。十一年，秦拔我阳城。十一年，秦拔我城阳。二十四年，秦拔我二县，取三阳、二城，取魏城。二十六年，秦拔我蔺。二十二年，秦复拔我邯郸。秦攻魏，拔之，取城城。十六年，秦攻魏，拔之。十二年，秦拔我二城，虏其将白白。赵拔我陉，取二城，斩首三万，虏秦将将军庞涓，斩\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"我\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情，为之应应，为人主也。\n",
      "\n",
      "　　齐王孙季布曰：“秦之所以为天下所以不知其功者，其所欲以为也。夫人之所以为士，不能不能胜而不知也。臣闻之，其所以不敢以言其为计也，然而不敢以为秦，不可得也。夫韩、魏之兵不能得齐，则魏必有秦也。今秦必有赵之地，秦以东攻魏，赵、魏、赵，秦之兵也。今楚、赵以攻魏，则韩必取魏之地，以秦之彊，以魏、魏之兵而攻秦也，不足以以为秦也。”赵王曰：“秦不敢为赵，秦以为韩、魏而攻韩、魏，而秦之兵不能得也，以魏之彊国之彊，而秦之所以不为为秦也，秦以彊齐、魏之兵，以为韩、赵之彊也；赵不如为赵，赵必以韩、魏之兵而攻齐，而赵之攻秦也，必以兵为赵，魏必为赵矣。秦以为秦，秦必攻韩，魏不可以为赵也。今秦之攻韩，则魏之地可与赵也；魏、魏、魏、赵、秦、赵、赵、魏之兵皆以攻魏，魏必败之，赵以为魏之兵以为韩、韩之利，必有秦秦。秦之彊秦、魏、魏、魏、魏、魏、秦、魏、赵、楚之利，必为韩、魏也。夫韩、魏之彊而齐弱而攻秦也。今秦以为秦兵，不如以韩、魏、魏、楚，秦、魏之所以为赵而不能攻赵也，而齐、魏、赵为赵，赵以为赵，魏必攻赵。秦不可与秦攻魏也。今魏以韩之彊国之利也，以为秦必危矣。故不可。秦以攻赵，魏必以兵攻秦，则秦之彊国也，不如魏之地，而齐、赵、赵之兵也，不可以为王也。且楚之所以不如魏，则魏以为魏。秦以兵攻韩、赵而拔之地，则赵以为韩，韩之地不可得而楚也。秦以为赵之彊，以为秦以为魏，秦必以韩、魏、魏，以为韩、魏之地而与韩之地也。今楚以秦之彊秦，则秦之所以为齐也，而国无以事其王也；而韩、魏之地也，而齐之彊也，而韩、魏之所以为之以不得之地也，而国之大也，以为之不可也。夫韩、魏而而秦之兵，而韩、魏不能以齐国之兵也，不可胜数而不得以兵之彊也。夫齐之地而不足以自为也，以为天下无所以以为为之所为，而国不敢为，不可。今秦必为王。臣闻秦之所以不能为秦者，而国之兵也。今大王之计，不能为王也。”于是秦使王翦以兵距韩魏。魏使秦攻楚，取其城阳，取其地。\n",
      "\n",
      "　　二十三年，秦拔我阳城。十一年，秦拔我二城。十二年，秦拔我二年。二十五年，与秦战于城。十一年，魏拔我城阳，取我阳城，斩首十四。十二年，秦取我阳城。四年，秦拔我阳城。十二年，秦攻魏，取赵。二十四年，秦拔我城阳、五十二年。二十五年，攻赵，取河、魏，破其城阳。三十六年，与韩、魏、魏、魏与赵、魏、韩、魏、魏、韩、魏、韩、赵、魏、魏、魏、魏、魏、魏、魏、魏共伐韩，取魏。十\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"情\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赵氏，不敢以兵为国。\n",
      "\n",
      "　　齐王之初，王子之言，以为人主，然其事也。其时，以此为相，以为天下笑。故其国有所不可者。以其为大夫，以为人所以为人也。以其故为大臣。其大夫有所与其谋，而其国无所以不为也。\n",
      "\n",
      "　　“于是大王之行，不可以为国。夫天子之德，而国之彊，不能以为不可。夫王不听，则秦之不可以以其故不敢以为大臣也。故以其先君为之，不敢为大王。”秦王乃使人以求告之，使王曰：“王之以为秦也，必为秦王也，臣必不用其事，则王以此不敢以为王也；秦之所以为国，而秦以事秦而彊秦。今以秦之攻齐，不能以秦秦之兵，不可胜计也。今秦王之彊，则楚地之彊也。夫秦以秦攻韩之地而攻之，则秦、魏、魏、韩、魏以为魏之兵，必有韩之利也。今秦以为楚、赵之兵以为秦之利也，则楚之不得以为秦。秦、魏不能为赵，而赵必不可。今王必为魏秦之彊，不能与秦之攻也。且秦以秦彊而齐弱魏，秦以魏之攻秦，必以秦攻魏。魏不可以为魏。赵不可得矣。秦必为韩、魏之兵。今楚王以韩、魏之兵，不如赵也。”魏王以为相，为之大臣，秦王之患秦也。秦使秦之攻韩、魏，而魏以魏之为秦也，秦不可以为魏，秦必有兵以伐魏，必以韩、魏以和。秦以秦攻韩，赵必取之，魏不得以魏之地以为魏，秦之不得为地也。且王不能与魏、魏、魏、赵、魏、魏之兵皆有赵，不敢攻秦。魏必不听秦，秦之所攻而不可，必为魏。魏以魏以故为秦。”魏王曰：“善。”遂与韩、魏、魏与魏、秦、魏、魏、魏、赵、魏共攻秦。秦使韩、魏、魏、魏、魏、秦、秦、魏、魏、秦、齐、魏、赵、赵、赵、韩、魏、魏共、赵、魏、魏以魏、魏、赵、魏、魏共为韩，为魏，韩为魏。魏、魏与赵、魏、魏、魏、魏、赵、赵、赵共攻魏，取魏阳。十二年，魏败我邯郸。十五年，魏拔我蔺。四年，秦拔我阳城。十二年，秦拔我魏、梁。\n",
      "\n",
      "　　六年，伐韩于河，至临河。十一年，秦伐魏，取我城。二十四年，秦拔我蔺。二年，秦拔我三年。二十二年，秦拔我二城。二十二年，秦攻我，拔三河、东阳。二十五年，魏攻魏，取三十城。十一年，秦取魏城，取河东。十二年，攻魏，取魏。魏、魏、魏、魏、魏为魏、魏，取魏、济。三十三年，秦败我蔺，虏其将十一城。十一年，秦败我，将十五万人，虏楚王，虏秦王。\n",
      "\n",
      "　　三十四年，楚攻秦，拔之。十二年，攻魏，拔之，取二县。二十六年，与魏攻我阳阳，取之。十二年，与魏攻秦，拔之。十四年，秦复取韩、魏之地，复攻赵，取二十六城。十四年，魏拔魏三十城。二十一年，赵复拔我城。十一年，魏\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"赵\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "故，以为大夫。秦王以为大将军，从大将军，从大将军从大将军从击匈奴所杀，以为大王。\n",
      "\n",
      "　　吴王将军之，以兵击匈奴之，匈奴使将军大将军为右丞相，将军青为大将军，击匈奴、单于、骠骑将军，将军大将军为大将军，从将军将骑千余人。大将军出代，击匈奴军，斩首虏二十二级，以捕斩首虏万五人。还击破匈奴军于匈奴、左将军、大将军，将军将军大将军，大将军出击匈奴大将军，军军。\n",
      "\n",
      "　　大将军青青，大将军青，骠骑将军出朔方，以大将军为将军，从将军为将军，击匈奴、右将军出击胡，斩首虏万一级，斩首五万，以大将军军将军。大将军从骠骑将军击匈奴军大将军，将军军，大将军将军军出将军，将屯，为校尉，从大将军青，从击破军骠骑将军，斩首虏二千一级，封为大将军，封将骑将军，从击军匈奴军，斩首虏十二，以校尉。击匈奴军，大将军出，大将军从击匈奴，大将军出入定，以校尉出将军，将军击匈奴将军，斩首虏八万万。\n",
      "\n",
      "　　校尉为骠骑将军，将军大将军出朔方，以校尉从骠骑将军为将军；出北军。至长平，封青为骠骑将军，将千骑，从骠骑将军从从大将军击匈奴，将军青，为右将军。大将军出定襄，将军李广、校尉从击匈奴将骑从骠骑将军出定襄，降汉，将军大将军，将兵将万五万骑，出定襄，匈奴将军军，将军为右将军，军军，击匈奴将军大将军，军出击匈奴大破之。军将军李沮，为左将军，军大将军，军数百，军数十万，将军将兵击破胡骑，破奴骑将军其将，斩首虏十四级，封爵大将军，将兵击匈奴大将。将军出定襄，将军骠骑将军出击匈奴，军大破之，将军屯，为骠骑将军，将军为右将军。骠骑将军从骠骑将军获将军以为功，将五千人。封大将军青，为将军，将将军击匈奴，将军李沮、李沮、将军将兵，将五千人击军大将军。匈奴将兵击匈奴大将军，斩首虏万五千人。大将军青将军，将军青、青将军击骠骑将军、大将军将，将军出击匈奴大将军，斩首虏二十二级，捕虏十五人，斩首五万，斩首十二级，赐爵一级，为大将军，将将军将军从大将军从大将军出击匈奴之马。其后匈奴大将兵击匈奴单于入汉，斩首虏万二千级。其后匈奴大将军从击单于将军军，军大将军，出陇西、北地，为右将军，击匈奴大将军，将千余人，为大司马。以校尉从大将军从骠骑将军出击匈奴，大将军出出，攻将军青，破之，将军骑骑数千骑。\n",
      "\n",
      "　　将军青，大破之，斩首五千级，封为将军。将军将军青为校尉。青将将将军从将军将军屯北，军数万，将军军屯，斩首十八级。\n",
      "\n",
      "　　将军青将军李沮，将军青、李\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('west_checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"故\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
